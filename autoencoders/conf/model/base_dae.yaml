# Defining the optimizer as a group default allows CLI override, e.g.
# python train.py "optimizer@model.optimizer=sgd"
# or via config "override scheduler@model.scheduler: cyclic"
# See https://stackoverflow.com/questions/71438040/overwriting-hydra-configuration-groups-from-cli/71439510#71439510
defaults:
  - /optimizer@optimizer: adam
  - /scheduler@scheduler: cyclic

name: DAE

nn:
  _target_: autoencoders.models.base_dae.DenoisingAutoEncoder
  layers: [128, 64, 64]
  input_shape: [28, 28]
  loss_func:
    _target_: torch.nn.MSELoss