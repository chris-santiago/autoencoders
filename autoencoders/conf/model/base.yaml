# Defining the optimizer as a group default allows CLI override, e.g.
# python train.py "optimizer@model.optimizer=sgd"
#
# See https://stackoverflow.com/questions/71438040/overwriting-hydra-configuration-groups-from-cli/71439510#71439510
defaults:
  - /optimizer@optimizer: adam
  - /scheduler@scheduler: plateau

name: autoencoder

nn:
  _target_: autoencoders.base.AutoEncoder
  layers: [128, 64, 16]
  input_shape: [28, 28]
  loss_func:
    _target_: torch.nn.MSELoss