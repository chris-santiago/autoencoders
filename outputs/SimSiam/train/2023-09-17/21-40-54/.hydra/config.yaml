data:
  batch_size: 256
  n_workers: 8
  name: mnist
  train:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: autoencoders.data.SimSiamDataset
      dataset:
        _target_: autoencoders.data.get_mnist_dataset
        train: true
      augment_1:
        _target_: torchvision.transforms.GaussianBlur
        kernel_size: 3
      augment_2:
        _target_: torchvision.transforms.RandomPerspective
        distortion_scale: 0.75
        p: 1.0
    batch_size: ${data.batch_size}
    shuffle: true
    num_workers: ${data.n_workers}
  valid:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: autoencoders.data.SimSiamDataset
      dataset:
        _target_: autoencoders.data.get_mnist_dataset
        train: true
      augment_1:
        _target_: torchvision.transforms.ElasticTransform
        alpha: 100.0
      augment_2:
        _target_: torchvision.transforms.RandomPerspective
        distortion_scale: 0.75
        p: 1.0
    batch_size: ${data.batch_size}
    shuffle: false
    num_workers: ${data.n_workers}
model:
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 10
  name: SimSiam
  nn:
    _target_: autoencoders.models.simsiam.SimSiam
    encoder:
      _target_: autoencoders.modules.CNNEncoderProjection
      channels_in: 1
      base_channels: 32
      latent_dim: ${model.nn.dim}
    dim: 512
    pred_dim: 512
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 200
  accelerator: mps
  devices: 1
  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: autoencoders
    name: null
    id: null
    group: null
    job_type: null
    save_dir: ${hydra:runtime.output_dir}
    log_model: true
    tags: ${tags}
callbacks:
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
  progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
    refresh_rate: 5
    leave: true
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: train-loss
    min_delta: 0.0001
    patience: 10
    check_on_train_epoch_end: true
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${hydra:runtime.output_dir}/checkpoints
    monitor: train-loss
    save_top_k: 1
    save_on_train_epoch_end: true
tags:
- ${data.name}
- ${model.name}
